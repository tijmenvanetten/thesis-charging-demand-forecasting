{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_target, load_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from darts.models import TCNModel\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import optuna\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from darts.metrics import smape\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "from darts import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(series, train_split: float, val_split: float):\n",
    "    val_len = int(len(series) * train_split)\n",
    "    test_len = int(len(series) * val_split)\n",
    "    train, val, test = series[:val_len], series[val_len:test_len], series[test_len:]\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "target_series = load_target('../../data/03_processed/on_forecourt_sessions.csv', group_cols='location_id',\n",
    "                            time_col='date', value_cols='energy_delivered_kwh', static_cols=['num_evse'], freq='D')\n",
    "covariates = load_covariates('../../data/03_processed/weather_ecad.csv', time_col='date',\n",
    "                                value_cols=['temp_max', 'temp_min', 'sunshine', 'precip'], freq='D')\n",
    "\n",
    "target_series = [series for series in target_series if len(series) == 1035]\n",
    "# Cluster Time Series\n",
    "series = concatenate(target_series, axis=1)\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.85\n",
    "\n",
    "train_series, val_series, test_series = train_val_test_split(series, TRAIN_SPLIT, VAL_SPLIT)\n",
    "\n",
    "# scale target\n",
    "target_scaler = Scaler(MaxAbsScaler())\n",
    "train_series = target_scaler.fit_transform(train_series)\n",
    "val_series = target_scaler.transform(val_series)\n",
    "series_transformed = target_scaler.transform(series)\n",
    "\n",
    "\n",
    "train_covariates, val_covariates, test_covariates = train_val_test_split(covariates, TRAIN_SPLIT, VAL_SPLIT)\n",
    "# scale covariate\n",
    "covariate_scaler = Scaler(MaxAbsScaler())\n",
    "train_covariates = covariate_scaler.fit_transform(train_covariates)\n",
    "val_covariates = covariate_scaler.transform(val_covariates)\n",
    "covariates_transformed = covariate_scaler.transform(covariates)\n",
    "\n",
    "train_val_series = concatenate([train_series, val_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective function\n",
    "def objective(trial):\n",
    "    # select input and output chunk lengths\n",
    "    in_len = trial.suggest_int(\"in_len\", 12, 36)\n",
    "    out_len = trial.suggest_int(\"out_len\", 1, in_len-1)\n",
    "\n",
    "    # Other hyperparameters\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 1, 5)\n",
    "    weight_norm = trial.suggest_categorical(\"weight_norm\", [False, True])\n",
    "    dilation_base = trial.suggest_int(\"dilation_base\", 2, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 5e-5, 1e-3, log=True)\n",
    "    include_day = trial.suggest_categorical(\"day\", [False, True])\n",
    "\n",
    "    # throughout training we'll monitor the validation loss for both pruning and early stopping\n",
    "    # pruner = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.01, patience=3, verbose=True)\n",
    "\n",
    "    pl_trainer_kwargs = {\"callbacks\": [ early_stopper]}\n",
    "    num_workers = 0\n",
    "\n",
    "    # optionally also add the (scaled) year value as a past covariate\n",
    "    if include_day:\n",
    "        encoders = {\"datetime_attribute\": {\"past\": [\"day\"]},\n",
    "                    \"transformer\": Scaler()}\n",
    "    else:\n",
    "        encoders = None\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # build the TCN model\n",
    "    model = TCNModel(\n",
    "        input_chunk_length=in_len,\n",
    "        output_chunk_length=out_len,\n",
    "        batch_size=32,\n",
    "        n_epochs=100,\n",
    "        nr_epochs_val_period=1,\n",
    "        kernel_size=kernel_size,\n",
    "        num_filters=num_filters,\n",
    "        weight_norm=weight_norm,\n",
    "        dilation_base=dilation_base,\n",
    "        dropout=dropout,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        add_encoders=encoders,\n",
    "        likelihood=GaussianLikelihood(),\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"tcn_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    model.fit(\n",
    "        series=train_series,\n",
    "        past_covariates=covariates_transformed,\n",
    "        val_series=val_series,\n",
    "        val_past_covariates=covariates_transformed,\n",
    "        num_loader_workers=num_workers,\n",
    "    )\n",
    "\n",
    "   \n",
    "    # reload best model over course of training\n",
    "    model = TCNModel.load_from_checkpoint(\"tcn_model\")\n",
    "\n",
    "    # Evaluate how good it is on the validation set, using sMAPE\n",
    "    # preds = model.predict(series=train, n=VAL_LEN)\n",
    "\n",
    "    smapes = model.backtest(\n",
    "        train_val_series,\n",
    "        start=val_series.start_time(),\n",
    "        forecast_horizon=1,\n",
    "        stride=1,\n",
    "        last_points_only=False,\n",
    "        retrain=False,\n",
    "        verbose=True,\n",
    "        metric=smape\n",
    "    )\n",
    "    smape_val = np.mean(smapes)\n",
    "\n",
    "    return smape_val if smape_val != np.nan else float(\"inf\")\n",
    "\n",
    "\n",
    "# for convenience, print some optimization trials information\n",
    "def print_callback(study, trial):\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n",
    "\n",
    "\n",
    "# optimize hyperparameters by minimizing the sMAPE on the validation set\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = study.trials_dataframe()\n",
    "results[results['value'] == results['value'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
